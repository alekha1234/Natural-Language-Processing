{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28f6056d-2972-46c4-af6d-53ae46e6bd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop making me think about you! I'm busy.        Cuddling with you would be perfect right now ðŸ’‘        If nothing lasts forever, can I be your nothing?        You make my heart melt! ðŸ’•        I couldn't ignore you even if I wanted to.        Every moment away from you feels like a lifetime.        Whenever I am with you, it is like having my emotional batteries recharged with joy.         Your smile radiates into me. Your touch sends little shivers through my body. Your presence         pleases my mind, and your soul pours peace on mine. I love youâ€¦madly, sincerely, completely,         and with no reservation,in a way that is blissfully wonderful.\n"
     ]
    }
   ],
   "source": [
    "my_text = \"Stop making me think about you! I'm busy.\\\n",
    "        Cuddling with you would be perfect right now ðŸ’‘\\\n",
    "        If nothing lasts forever, can I be your nothing?\\\n",
    "        You make my heart melt! ðŸ’•\\\n",
    "        I couldn't ignore you even if I wanted to.\\\n",
    "        Every moment away from you feels like a lifetime.\\\n",
    "        Whenever I am with you, it is like having my emotional batteries recharged with joy. \\\n",
    "        Your smile radiates into me. Your touch sends little shivers through my body. Your presence \\\n",
    "        pleases my mind, and your soul pours peace on mine. I love youâ€¦madly, sincerely, completely, \\\n",
    "        and with no reservation,in a way that is blissfully wonderful.\"\n",
    "\n",
    "\n",
    "print(my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a977fcd-13d3-4f35-83d7-6b7cab72f0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34744b83-997e-4514-94e3-b11a1a0ec4f8",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9ca098-4056-45b1-b881-0becaa7d88b6",
   "metadata": {},
   "source": [
    "## 1. Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c60dcdf4-e31e-4395-835e-e4adefdbd595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop making me think about you! i'm busy.        cuddling with you would be perfect right now ðŸ’‘        if nothing lasts forever, can i be your nothing?        you make my heart melt! ðŸ’•        i couldn't ignore you even if i wanted to.        every moment away from you feels like a lifetime.        whenever i am with you, it is like having my emotional batteries recharged with joy.         your smile radiates into me. your touch sends little shivers through my body. your presence         pleases my mind, and your soul pours peace on mine. i love youâ€¦madly, sincerely, completely,         and with no reservation,in a way that is blissfully wonderful.\n"
     ]
    }
   ],
   "source": [
    "# converting enrtire text into lower case due to case senitive\n",
    "\n",
    "lower_text = my_text.lower()\n",
    "print(lower_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264f705d-10fe-47ad-81b6-0205673d25e5",
   "metadata": {},
   "source": [
    "## 2. Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2766a889-e14a-4189-9b1d-eade8878e95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "#!pip install string\n",
    "import string\n",
    "from string import punctuation          # import punctuation from string or we can give a list manually\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0532146d-3fea-4cbf-8b7c-5896ee90784b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop making me think about you im busy        cuddling with you would be perfect right now ðŸ’‘        if nothing lasts forever can i be your nothing        you make my heart melt ðŸ’•        i couldnt ignore you even if i wanted to        every moment away from you feels like a lifetime        whenever i am with you it is like having my emotional batteries recharged with joy         your smile radiates into me your touch sends little shivers through my body your presence         pleases my mind and your soul pours peace on mine i love youâ€¦madly sincerely completely         and with no reservationin a way that is blissfully wonderful\n"
     ]
    }
   ],
   "source": [
    "# using for loop \n",
    "\n",
    "for punc in punctuation:\n",
    "    if punc in lower_text:\n",
    "        lower_text = lower_text.replace(punc ,\"\")\n",
    "print(lower_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b1e49-22ef-4e7b-9a30-7b1b936999c6",
   "metadata": {},
   "source": [
    "## 3. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf28ff84-5598-4888-acda-8019a8309f78",
   "metadata": {},
   "source": [
    "### i. Line Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bd085df5-6be5-4d62-af0d-2a1bc41fa8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gujur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "949fb386-811b-4cd2-b20b-28aed004e8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Stop making me think about you! I'm busy.        Cuddling with you would be perfect right now ðŸ’‘        If nothing lasts forever, can I be your nothing?        You make my heart melt! ðŸ’•        I couldn't ignore you even if I wanted to.        Every moment away from you feels like a lifetime.        Whenever I am with you, it is like having my emotional batteries recharged with joy.         Your smile radiates into me. Your touch sends little shivers through my body. Your presence         pleases my mind, and your soul pours peace on mine. I love youâ€¦madly, sincerely, completely,         and with no reservation,in a way that is blissfully wonderful.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import line_tokenize\n",
    "line_tokens = line_tokenize(text = my_text)\n",
    "print(line_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474d7bfd-731b-4a5a-baa1-a749a7af0315",
   "metadata": {},
   "source": [
    "### ii. Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7823ccf9-e00d-4a07-8210-7bc4d211d8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stop making me think about you!', \"I'm busy.\", 'Cuddling with you would be perfect right now ðŸ’‘        If nothing lasts forever, can I be your nothing?', 'You make my heart melt!', \"ðŸ’•        I couldn't ignore you even if I wanted to.\", 'Every moment away from you feels like a lifetime.', 'Whenever I am with you, it is like having my emotional batteries recharged with joy.', 'Your smile radiates into me.', 'Your touch sends little shivers through my body.', 'Your presence         pleases my mind, and your soul pours peace on mine.', 'I love youâ€¦madly, sincerely, completely,         and with no reservation,in a way that is blissfully wonderful.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokens = sent_tokenize(my_text , language='english')\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f87c46c-bc3e-474f-af35-1095f2381526",
   "metadata": {},
   "source": [
    "### iii. WhiteSpace Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4b19b021-9017-475f-8595-5e64f99e74b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stop', 'making', 'me', 'think', 'about', 'you', 'im', 'busy', 'cuddling', 'with', 'you', 'would', 'be', 'perfect', 'right', 'now', 'ðŸ’‘', 'if', 'nothing', 'lasts', 'forever', 'can', 'i', 'be', 'your', 'nothing', 'you', 'make', 'my', 'heart', 'melt', 'ðŸ’•', 'i', 'couldnt', 'ignore', 'you', 'even', 'if', 'i', 'wanted', 'to', 'every', 'moment', 'away', 'from', 'you', 'feels', 'like', 'a', 'lifetime', 'whenever', 'i', 'am', 'with', 'you', 'it', 'is', 'like', 'having', 'my', 'emotional', 'batteries', 'recharged', 'with', 'joy', 'your', 'smile', 'radiates', 'into', 'me', 'your', 'touch', 'sends', 'little', 'shivers', 'through', 'my', 'body', 'your', 'presence', 'pleases', 'my', 'mind', 'and', 'your', 'soul', 'pours', 'peace', 'on', 'mine', 'i', 'love', 'youâ€¦madly', 'sincerely', 'completely', 'and', 'with', 'no', 'reservationin', 'a', 'way', 'that', 'is', 'blissfully', 'wonderful']\n"
     ]
    }
   ],
   "source": [
    "# split all text based on white space\n",
    "space_token = lower_text.split()\n",
    "print(space_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02a1bf8-a6d8-49d9-929d-e92428edf813",
   "metadata": {},
   "source": [
    "### iv. Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5584659-bc1e-4201-a756-8504987ef999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stop', 'making', 'me', 'think', 'about', 'you', 'im', 'busy', 'cuddling', 'with', 'you', 'would', 'be', 'perfect', 'right', 'now', 'ðŸ’‘', 'if', 'nothing', 'lasts', 'forever', 'can', 'i', 'be', 'your', 'nothing', 'you', 'make', 'my', 'heart', 'melt', 'ðŸ’•', 'i', 'couldnt', 'ignore', 'you', 'even', 'if', 'i', 'wanted', 'to', 'every', 'moment', 'away', 'from', 'you', 'feels', 'like', 'a', 'lifetime', 'whenever', 'i', 'am', 'with', 'you', 'it', 'is', 'like', 'having', 'my', 'emotional', 'batteries', 'recharged', 'with', 'joy', 'your', 'smile', 'radiates', 'into', 'me', 'your', 'touch', 'sends', 'little', 'shivers', 'through', 'my', 'body', 'your', 'presence', 'pleases', 'my', 'mind', 'and', 'your', 'soul', 'pours', 'peace', 'on', 'mine', 'i', 'love', 'youâ€¦madly', 'sincerely', 'completely', 'and', 'with', 'no', 'reservationin', 'a', 'way', 'that', 'is', 'blissfully', 'wonderful']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokens = word_tokenize(text=lower_text ,language='english')\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76122795-70cc-4b99-b57f-629d5d0da13c",
   "metadata": {},
   "source": [
    "## 4. Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fae339ba-665d-473c-af88-4a89457c1c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gujur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download nltk stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e9c69175-ce0c-4dc2-b881-ae3f800e2a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# print nltk english stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "38723f4b-f101-4e8c-a1a0-4eed97bb5beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stop', 'making', 'think', 'im', 'busy', 'cuddling', 'would', 'perfect', 'right', 'ðŸ’‘', 'nothing', 'lasts', 'forever', 'nothing', 'make', 'heart', 'melt', 'ðŸ’•', 'couldnt', 'ignore', 'even', 'wanted', 'every', 'moment', 'away', 'feels', 'like', 'lifetime', 'whenever', 'like', 'emotional', 'batteries', 'recharged', 'joy', 'smile', 'radiates', 'touch', 'sends', 'little', 'shivers', 'body', 'presence', 'pleases', 'mind', 'soul', 'pours', 'peace', 'mine', 'love', 'youâ€¦madly', 'sincerely', 'completely', 'reservationin', 'way', 'blissfully', 'wonderful']\n"
     ]
    }
   ],
   "source": [
    "# filter words and create a new list without stopword\n",
    "filtered_words = []\n",
    "for word in word_tokens:\n",
    "    if word not in stopwords.words('english'):\n",
    "        filtered_words.append(word)\n",
    "\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b183040-057d-428c-af08-6df4ee9aecb2",
   "metadata": {},
   "source": [
    "## 5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ec9132c0-bd68-46ae-9052-5508e709c2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gujur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gujur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')  # for pos tager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "534491ff-29e6-42a3-8fd2-d89809e05a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stop', 'making', 'think', 'im', 'busy', 'cuddling', 'would', 'perfect', 'right', 'ðŸ’‘', 'nothing', 'last', 'forever', 'nothing', 'make', 'heart', 'melt', 'ðŸ’•', 'couldnt', 'ignore', 'even', 'wanted', 'every', 'moment', 'away', 'feel', 'like', 'lifetime', 'whenever', 'like', 'emotional', 'battery', 'recharged', 'joy', 'smile', 'radiates', 'touch', 'sends', 'little', 'shiver', 'body', 'presence', 'plea', 'mind', 'soul', 'pours', 'peace', 'mine', 'love', 'youâ€¦madly', 'sincerely', 'completely', 'reservationin', 'way', 'blissfully', 'wonderful']\n"
     ]
    }
   ],
   "source": [
    "# word lemmetizer without pos tags\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lemmitizer = WordNetLemmatizer()\n",
    "\n",
    "lemmitized_words = []\n",
    "for word in filtered_words:\n",
    "    lemmitized_words.append(word_lemmitizer.lemmatize(word))\n",
    "\n",
    "print(lemmitized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc1a53-1ab3-43df-9803-7365d6c25014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d765db9f-8e52-4010-ad59-5a9b4d84dc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('stop', 'v'), ('make', 'v'), ('think', 'a'), ('im', 'a'), ('busy', 'a'), ('cuddle', 'v'), ('would', 'n'), ('perfect', 'v'), ('right', 'a'), ('ðŸ’‘', 'n'), ('nothing', 'n'), ('last', 'v'), ('forever', 'r'), ('nothing', 'n'), ('make', 'a'), ('heart', 'n'), ('melt', 'n'), ('ðŸ’•', 'n'), ('couldnt', 'n'), ('ignore', 'r'), ('even', 'r'), ('want', 'v'), ('every', 'n'), ('moment', 'n'), ('away', 'r'), ('feel', 'n'), ('like', 'n'), ('lifetime', 'r'), ('whenever', 'n'), ('like', 'n'), ('emotional', 'a'), ('battery', 'n'), ('recharge', 'v'), ('joy', 'n'), ('smile', 'n'), ('radiate', 'v'), ('touch', 'a'), ('sends', 'n'), ('little', 'a'), ('shiver', 'n'), ('body', 'v'), ('presence', 'n'), ('plea', 'n'), ('mind', 'v'), ('soul', 'a'), ('pours', 'n'), ('peace', 'v'), ('mine', 'a'), ('love', 'n'), ('youâ€¦madly', 'r'), ('sincerely', 'r'), ('completely', 'r'), ('reservationin', 'a'), ('way', 'n'), ('blissfully', 'r'), ('wonderful', 'a')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Function to convert NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if no match\n",
    "\n",
    "# Lemmatize each word with its POS tag\n",
    "lemmatized_words = []\n",
    "for word, tag in nltk.pos_tag(filtered_words):\n",
    "    wordnet_pos = get_wordnet_pos(tag)  # Convert to WordNet POS\n",
    "    lemmatized_word = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "    lemmatized_words.append((lemmatized_word, wordnet_pos))\n",
    "\n",
    "# Output the lemmatized words with their POS tags\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3ec599-3154-4d7d-9652-24d70037ffe7",
   "metadata": {},
   "source": [
    "## 6. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b68fc4-2fdb-40f7-a26d-47704b0c1197",
   "metadata": {},
   "source": [
    "Examples of Stemming:\n",
    "```python\n",
    "running â†’ run\n",
    "jumps â†’ jump\n",
    "happily â†’ happi\n",
    "better â†’ better\n",
    "children â†’ child"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1095178-9ade-4354-86fd-87dbaabe0019",
   "metadata": {},
   "source": [
    "- There are four types of stemmer we are commonly using:-\n",
    "\n",
    "        - Porter Stemmer\n",
    "        - Lancaster Stemmer\n",
    "        - Snowball Stemmer\n",
    "        - Regexp Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2a391db1-1b18-4174-a58a-13e1b3a59751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stop', 'make', 'think', 'im', 'busi', 'cuddl', 'would', 'perfect', 'right', 'ðŸ’‘', 'noth', 'last', 'forev', 'noth', 'make', 'heart', 'melt', 'ðŸ’•', 'couldnt', 'ignor', 'even', 'want', 'everi', 'moment', 'away', 'feel', 'like', 'lifetim', 'whenev', 'like', 'emot', 'batteri', 'recharg', 'joy', 'smile', 'radiat', 'touch', 'send', 'littl', 'shiver', 'bodi', 'presenc', 'pleas', 'mind', 'soul', 'pour', 'peac', 'mine', 'love', 'youâ€¦madli', 'sincer', 'complet', 'reservationin', 'way', 'bliss', 'wonder']\n"
     ]
    }
   ],
   "source": [
    "# Porter Stemmer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "porter_words = []\n",
    "for word in filtered_words:\n",
    "    porter_words.append(stemmer.stem(word))\n",
    "\n",
    "print(porter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7a0c232a-d5f3-4934-8ed1-994bd5c2be76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stop', 'mak', 'think', 'im', 'busy', 'cuddl', 'would', 'perfect', 'right', 'ðŸ’‘', 'noth', 'last', 'forev', 'noth', 'mak', 'heart', 'melt', 'ðŸ’•', 'couldnt', 'ign', 'ev', 'want', 'every', 'mom', 'away', 'feel', 'lik', 'lifetim', 'whenev', 'lik', 'emot', 'battery', 'recharg', 'joy', 'smil', 'rady', 'touch', 'send', 'littl', 'shiv', 'body', 'pres', 'pleas', 'mind', 'soul', 'pour', 'peac', 'min', 'lov', 'youâ€¦madly', 'sint', 'complet', 'reservationin', 'way', 'bliss', 'wond']\n"
     ]
    }
   ],
   "source": [
    "# Lancaster Stemmer\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "lancaster_words = []\n",
    "for word in filtered_words:\n",
    "    lancaster_words.append(stemmer.stem(word))\n",
    "\n",
    "print(lancaster_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a53fd031-b5ab-4ca1-bc5f-3c2f189052ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stop', 'make', 'think', 'im', 'busi', 'cuddl', 'would', 'perfect', 'right', 'ðŸ’‘', 'noth', 'last', 'forev', 'noth', 'make', 'heart', 'melt', 'ðŸ’•', 'couldnt', 'ignor', 'even', 'want', 'everi', 'moment', 'away', 'feel', 'like', 'lifetim', 'whenev', 'like', 'emot', 'batteri', 'recharg', 'joy', 'smile', 'radiat', 'touch', 'send', 'littl', 'shiver', 'bodi', 'presenc', 'pleas', 'mind', 'soul', 'pour', 'peac', 'mine', 'love', 'youâ€¦mad', 'sincer', 'complet', 'reservationin', 'way', 'bliss', 'wonder']\n"
     ]
    }
   ],
   "source": [
    "# Snowball Stemmer\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "snowball_words = []\n",
    "for word in filtered_words:\n",
    "    snowball_words.append(stemmer.stem(word))\n",
    "\n",
    "print(snowball_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a611dfa8-9163-4c10-8f84-7960e27fb54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stop', 'mak', 'think', 'im', 'busy', 'cuddl', 'would', 'perfect', 'right', 'ðŸ’‘', 'noth', 'last', 'forever', 'noth', 'mak', 'heart', 'melt', 'ðŸ’•', 'couldnt', 'ignor', 'even', 'wanted', 'every', 'moment', 'away', 'feel', 'lik', 'lifetim', 'whenever', 'lik', 'emotional', 'batterie', 'recharged', 'joy', 'smil', 'radiate', 'touch', 'send', 'littl', 'shiver', 'body', 'presenc', 'please', 'mind', 'soul', 'pour', 'peac', 'min', 'lov', 'youâ€¦madly', 'sincerely', 'completely', 'reservationin', 'way', 'blissfully', 'wonderful']\n"
     ]
    }
   ],
   "source": [
    "# Regexp Stemmer\n",
    "\n",
    "from nltk.stem import RegexpStemmer\n",
    "stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "\n",
    "regxp_words = []\n",
    "for word in filtered_words:\n",
    "    regxp_words.append(stemmer.stem(word))\n",
    "\n",
    "print(regxp_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421cb662-c61e-473a-ad19-cfea5ebf9ee8",
   "metadata": {},
   "source": [
    "## 7. Handling Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d0280519-ea1b-4660-bfef-de764299ab30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 --> ninth\n",
      "24 --> twenty-fourth\n",
      "567 --> five hundred and sixty-seventh\n",
      "8910 --> eight thousand, nine hundred and tenth\n"
     ]
    }
   ],
   "source": [
    "#!pip install num2words\n",
    "from num2words import num2words\n",
    "numbers = [9 , 24 , 567, 8910]\n",
    "\n",
    "for num in numbers:\n",
    "    print(f\"{num} --> {num2words(number=num ,to='ordinal' ,lang='en')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586136ee-6f09-4fb7-a4c2-92f52adb2aa7",
   "metadata": {},
   "source": [
    "## 8. Handling Negations / contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "826a6a0c-bf28-405e-bb84-61c8ee189439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am', 'Would not', 'Cannot', \"Could'v\", 'He is', \"You'r\"]\n"
     ]
    }
   ],
   "source": [
    "#!pip install contractions\n",
    "\n",
    "import contractions\n",
    "\n",
    "my_words = [\"I'm\" , \"Wouldn't\" ,\"Can't\" , \"Could'v\" , \"He's\" , \"You'r\"]\n",
    "\n",
    "correct_word = []\n",
    "for word in my_words:\n",
    "    correct_word.append(contractions.fix(word))\n",
    "\n",
    "print(correct_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd8bf4-52ed-4936-a71f-cea6f37f7d67",
   "metadata": {},
   "source": [
    "## 9. Removing Extra Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b2a27a38-0fc0-432c-aa9b-ac466a281777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original :    This   is      an      example    text   with   extra   spaces.  \n",
      "Cleaned :  This is an example text with extra spaces.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def remove_extra_whitespace(text):\n",
    "    # Remove leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "text = \"  This   is      an      example    text   with   extra   spaces.  \"\n",
    "cleaned_text = remove_extra_whitespace(text)\n",
    "print(\"Original : \",text)\n",
    "print(\"Cleaned : \",cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e4c40-f02f-4123-be51-7be0d0fdfe94",
   "metadata": {},
   "source": [
    "## 10. Correcting Misspellings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a25df3-2763-4807-a589-d0f2cfad9d63",
   "metadata": {},
   "source": [
    "```python\n",
    "recieve ---> receive\n",
    "definately ---> definitely\n",
    "ocurrance ---> occurrence\n",
    "seperate ---> separate\n",
    "acommodate ---> accommodate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c95ee80e-04c4-448c-9e44-d00aa43f9833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "959bfb56-f968-4e44-923a-66a5b362035c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accommodate', 'definitely', 'separate', 'receive', 'occurrence', 'calender', 'existence']\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "misspelled_words = [\"acomodate\",\"definately\",\"seperate\",\"recieve\",\"occurrance\",\"calender\", \"existance\"]\n",
    "corrected_words = []\n",
    "\n",
    "for word in misspelled_words:\n",
    "    corrected_words.append(spell.correction(word))\n",
    "\n",
    "print(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93234cf-524c-4eda-908b-400ddb043d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
